# ğŸ¦ Credit Risk Feature Store Tutorial using Feast, dbt, PostgreSQL & Redis

This project is a complete, production-style walkthrough of how to build a **real-time feature store** using [Feast](https://docs.feast.dev/), [dbt](https://docs.getdbt.com/), and the **Lending Club loan dataset**.

It demonstrates the full journey:

* Fetching raw data from Kaggle
* Cleaning and transforming data using Python + dbt
* Loading into PostgreSQL
* Materializing features with Feast into Redis
* Performing real-time lookups with just a few lines of code

---

## ğŸ“Š Dataset Used

* **Source**: [Kaggle - Lending Club](https://www.kaggle.com/datasets/wordsforthewise/lending-club)
* **Years Covered**: 2007â€“2018
* **Objective**: Use individual loan data to create features for credit risk models such as:

  * Loan amount
  * Interest rate
  * FICO score (bucketized)
  * State-level reject counts (geo-aggregates)

Due to the lack of `customer_id`, all features are engineered at the **loan level**, using `loan_id` as the primary entity.

---

## ğŸ“‚ Project Structure

```
credit-risk-feature-store-tutorial/
â”œâ”€â”€ data/                      # Raw and filtered dataset handlers
â”‚   â”œâ”€â”€ raw/                   # Original Kaggle files
â”‚   â”œâ”€â”€ raw_filtered/          # Cleaned and usable files
â”‚   â”œâ”€â”€ kaggle_fetch.py        # Downloads data using Kaggle API
â”‚   â””â”€â”€ filter_data.py         # Preprocesses and filters necessary columns
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ init/                  # SQL for schema creation if needed
â”‚   â””â”€â”€ load_to_postgres.py    # Loads cleaned CSVs into Postgres
â”‚
â”œâ”€â”€ dbt/                       # dbt project (created via `dbt init`)
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ staging/           # Cleaned models from raw tables
â”‚   â”‚   â””â”€â”€ features/          # Final models consumed by Feast
â”‚   â”œâ”€â”€ macros/                # Custom macros for feature logic
â”‚   â”œâ”€â”€ seeds, snapshots, etc.
â”‚   â””â”€â”€ dbt_project.yml
â”‚
â”œâ”€â”€ lending_club_features/     # Feast repo
â”‚   â”œâ”€â”€ feature_store.yaml     # Feast config (Postgres + Redis)
â”‚   â””â”€â”€ feature_repo/
â”‚       â”œâ”€â”€ entities.py
â”‚       â”œâ”€â”€ feature_views.py
â”‚       â””â”€â”€ feature_services.py
â”‚
â”œâ”€â”€ notebook/                  # Jupyter notebooks (if any)
â”œâ”€â”€ logs/                      # Logs or experiment metadata
â”œâ”€â”€ .env                       # Environment variables
â”œâ”€â”€ docker-compose.yml         # Postgres + Redis services
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ query_online.py            # Feature lookup script using Redis
```

---

## âš™ï¸ How Each Script Works

| Script / Component     | Purpose                                                 |
| ---------------------- | ------------------------------------------------------- |
| `kaggle_fetch.py`      | Pulls Lending Club data from Kaggle using API           |
| `filter_data.py`       | Cleans + formats data for modeling                      |
| `load_to_postgres.py`  | Loads data into PostgreSQL (offline store)              |
| `dbt/models/staging/`  | Cleans + standardizes schema                            |
| `dbt/models/features/` | Generates feature tables (used by Feast)                |
| `feature_repo/*.py`    | Defines entities, feature views, and services for Feast |
| `query_online.py`      | Runs real-time lookup from Redis using Feast            |

---

## ğŸ³ Cloning the Repository

```bash
git clone https://github.com/venkateshtantravahi/credit-risk-feature-store-tutorial.git
cd credit-risk-feature-store-tutorial
```

---

## ğŸ”§ Setting Up the Environment

### 1. ğŸ” Configure `.env`

Create a `.env` file in the root directory with the following:

```env
POSTGRES_USER=admin
POSTGRES_PASSWORD=admin
POSTGRES_DB=credit_db
POSTGRES_PORT=5432
REDIS_PORT=6379
KAGGLE_USERNAME=<your-kaggle-username>
KAGGLE_KEY=<your-kaggle-api-key>
```

> Make sure Kaggle API access is set up: [https://www.kaggle.com/docs/api](https://www.kaggle.com/docs/api)

### 2. ğŸšª Start Docker containers

```bash
docker-compose up -d
```

This runs:

* `postgres:14.9` on `${POSTGRES_PORT}`
* `redis:6.2` on `${REDIS_PORT}`

### 3. ğŸ Create and activate Python environment

```bash
conda create -n feast_feature_store python=3.11
conda activate feast_feature_store

pip install -r requirements.txt
```

---

## ğŸ”® Running the Pipeline

### 1. ğŸ“‚ Load and process data

```bash
python data/kaggle_fetch.py
python data/filter_data.py
python db/load_to_postgres.py
```

### 2. ğŸ› ï¸ Set up dbt

```bash
cd dbt
dbt init .
dbt build
```

### 3. ğŸ”  Apply Feast definitions

```bash
cd lending_club_features
feast apply
```

### 4. ğŸ•°ï¸ Materialize features

```bash
feast materialize 2007-01-01T00:00:00 2011-12-31T23:59:59
```

### 5. ğŸš€ Query online features

```bash
python query_online.py
```

> Ensure you're using a valid `loan_id` and `state` present in the dataset.

---

## ğŸ”„ Customization Tips

* âœ… Add new features inside `dbt/models/features/`
* âœ… Extend entities (e.g., state-level aggregations)
* âœ… Use `feast materialize-incremental` for incremental updates
* âœ… Switch online/offline stores (Redis â†’ DynamoDB, Postgres â†’ BigQuery)

---

## ğŸ“Œ Final Thoughts

This project provides a real-world example of combining modern data engineering tools to power machine learning feature workflows.

> From raw data to real-time serving â€” this tutorial gives you an end-to-end pipeline, built on open-source technologies.

---

## ğŸ“‹ Resources

* ğŸ“˜ [Feast Docs](https://docs.feast.dev/)
* ğŸ“˜ [dbt Docs](https://docs.getdbt.com/)
* ğŸƒ [PostgreSQL](https://www.postgresql.org/)
* ğŸ¨  [Redis](https://redis.io/)
* ğŸ“¦ [Kaggle Lending Club Dataset](https://www.kaggle.com/datasets/wordsforthewise/lending-club)

---

## ğŸ§‘â€ğŸ’» Author

Built by [Venkatesh Tantravahi](https://github.com/venkateshtantravahi) â€” feel free to open an issue or PR if you'd like to collaborate or improve this repo!
