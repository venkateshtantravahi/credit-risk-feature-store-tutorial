"""
This script provides a robust, JSON-driven mechanism for loading raw CSV data
into a Postgres SQL database. It is designed to be idempotent and re-runnable.

Key functionalities include:
- Reading schema definitions from JSON files generated by a data fetcher.
- Normalizing and sanitizing table and column names.
- Creating database schemas and tables if they don't exist. All columns in the
  raw layer are created as TEXT to ensure robust loading of messy data.
- Streaming large CSV or gzipped CSV files into Postgres SQL using the efficient
  COPY command, treating empty strings as NULL.
- Handling potential discrepancies between CSV headers and table schemas.
"""

import os
import csv
import gzip
import json
import re
import io
import logging
from pathlib import Path
from typing import List, Tuple, Generator, Optional, TextIO, Iterator, Dict

from sqlalchemy import create_engine, text, Engine
from sqlalchemy.exc import SQLAlchemyError
from dotenv import load_dotenv

load_dotenv()

# Configuration
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    datefmt="%Y-%m-%d %H:%M:%S",
)

# Constants
PROJECT_ROOT = Path(os.getenv("PROJECT_ROOT"))
RAW_DIR = PROJECT_ROOT / "data" / "raw_filtered"
METADATA_DIR = PROJECT_ROOT / "data" / "metadata"


# Custom Exceptions
class PostgresLoaderError(Exception):
    """Base exception for errors in this script"""

    pass


class DatabaseConnectionError(PostgresLoaderError):
    """Raised when a database connection fails"""

    pass


class SchemaValidationError(PostgresLoaderError):
    """Raised when a schema validation fails"""

    pass


class FileLoadError(PostgresLoaderError):
    """Raised when a file load fails during COPY operation"""

    pass


# Helper Classes
class IterableTextIO(io.TextIOBase):
    """
    A helper class that wraps a string iterator (like a generator) to make it
    behave like a file-like object with a `read()` method, which is required
    for psycopg2's `copy_expert` command.
    """

    def __init__(self, iterator: Iterator[str]) -> None:
        self._iterator = iterator
        self._buffer = ""

    def readable(self) -> bool:
        return True

    def read(self, size: int = -1) -> str:
        """
        Read from the iterator until the buffer has enough data or
        the iterator is exhausted.
        :param size:
        :return:
        """
        if size == -1:
            # Read until the iterator is exhuasted
            chunk = self._buffer + "".join(self._iterator)
            self._buffer = ""
            return chunk

        while len(self._buffer) < size:
            try:
                self._buffer += next(self._iterator)
            except StopIteration:
                break

        chunk, self._buffer = self._buffer[:size], self._buffer[size:]
        return chunk

    def readline(self) -> Optional[str]:
        """Read a single line from the iterator."""
        try:
            return next(self._iterator)
        except StopIteration:
            return ""


# Data Loader Class
class PostgresLoader:
    """
    Handles the end-to-end process of loading data into Postgres SQL
    based on JSON schema definitions.
    """

    def __init__(self, engine: Engine) -> None:
        self.engine = engine

    def ensure_db_schema_exists(self, schema_names: List[str]) -> None:
        """
        Ensures that the specified schema names exist.
        :param schema_names: A list of schema names to create if they don't exist.
        :return:
        """
        logging.info(f"Ensuring database schema exist {schema_names}")
        try:
            with self.engine.begin() as conn:
                for schema_name in schema_names:
                    conn.execute(
                        text(
                            f"CREATE SCHEMA IF NOT EXISTS {self._quote_ident(schema_name)}"
                        )
                    )
            logging.info("Database schema are ready.")
        except SQLAlchemyError as e:
            logging.error(f"Failed to create database schemas: {e}")
            raise DatabaseConnectionError("Failed to create database schemas.") from e

    def load_all_tables(self):
        """
        Orchestrates the loading process for all tables defined in the metadata directory.
        :return:
        """
        schema_jsons = sorted(METADATA_DIR.glob("*_schema.json"))
        if not schema_jsons:
            raise FileNotFoundError(
                f"No schema metadata found in {METADATA_DIR}. Run the data fetcher first."
            )

        all_data_files = self._list_tabular_files()
        if not all_data_files:
            raise FileNotFoundError(f"No CSV or CSV.GZ files found in {RAW_DIR}.")

        for sjson_path in schema_jsons:
            try:
                self.process_schema_file(sjson_path, all_data_files)
            except (SchemaValidationError, FileLoadError) as e:
                logging.warning(f"Skipping table due to an error: {e}")
                continue

    def process_schema_file(self, sjson_path: Path, all_data_files: List[Path]) -> None:
        """
        Processes a single schema JSON file, creates the corresponding table,
        and loads all matching data files
        :param sjson_path: Path to the schema JSON file.
        :param all_data_files: A list of all available data files to match against.
        :return:
        """
        table_name, schema_cols = self._read_and_parse_schema(sjson_path)
        logging.info(
            f"Processing schema {sjson_path.name} for table raw.{table_name} ({len(schema_cols)} columns)"
        )

        self._ensure_table_exists("raw", table_name, schema_cols)

        candidate_files = self._find_matching_files(
            str(table_name), sjson_path.name, all_data_files
        )
        if not candidate_files:
            logging.warning(
                f"No data files found matching table {table_name}. Skipping."
            )
            return

        for file_path in candidate_files:
            logging.info(f"Loading data from {file_path.name} into raw {table_name}")
            try:
                self._copy_file_into_table(
                    "raw", table_name, list(schema_cols.keys()), file_path
                )
            except FileLoadError as e:
                logging.error(f"Failed to load {file_path.name} into {table_name}: {e}")
                continue

    def _ensure_table_exists(
        self, schema: str, table: Path, cols: Dict[str, str]
    ) -> None:
        """
        Creates a table in the database if it does not already exist.
        ** CRITICAL **: All columns are created as TEXT to ensure robust loading.
        Type casting should be handled in a downstream transformation layer
        :param schema: The database schema name.
        :param table: The table name.
        :param cols: A dictionary mapping column names to their SQL types (types are ignored).
        :return:
        """
        # Force all columns to TEXT for robust rae data loading.
        cols_sql = ",\n  ".join([f"{self._quote_ident(c)} TEXT" for c in cols.keys()])
        ddl = f"CREATE TABLE IF NOT EXISTS {self._quote_ident(schema)}.{self._quote_ident(str(table))} (\n  {cols_sql}\n);"
        try:
            with self.engine.begin() as conn:
                conn.execute(text(ddl))
        except SQLAlchemyError as e:
            logging.error(f"Failed to execute DDL for table {schema}.{table}: {e}")
            raise DatabaseConnectionError(
                f"Could not create table {schema}.{table}: {e}"
            ) from e

    def _copy_file_into_table(
        self, schema: str, table: Path, columns: List[str], path: Path
    ) -> None:
        """
        Uses PostgreSQL's COPY command to efficiently stream data from a file into table.
        :param schema: The database schema name.
        :param table: The table name.
        :param columns: An ordered list of column names in the target table.
        :param path: The path to the CSV ot gzipped CSV file.
        :return:
        """
        opener = gzip.open if path.name.lower().endswith(".csv.gz") else open
        qcols = ", ".join([self._quote_ident(c) for c in columns])
        # USE NULL '' to handle empty strings as NULLs, which is safer.
        copy_sql = f"COPY {self._quote_ident(schema)}.{self._quote_ident(str(table))} ({qcols}) FROM STDIN WITH (FORMAT csv, HEADER true, NULL '')"
        raw_conn, cur = None, None
        try:
            with opener(path, "rt", encoding="utf-8", errors="ignore", newline="") as f:
                # Get the raw connection manually, outside a `with` block
                raw_conn = self.engine.raw_connection()
                # Get the cursor
                cur = raw_conn.cursor()
                # Wrap the generator in our file-like object
                copy_gen = self._create_copy_generator(f, columns)
                iterable_file = IterableTextIO(copy_gen)
                # Execute the copy operation
                cur.copy_expert(copy_sql, iterable_file)
                # Commit the transaction
                raw_conn.commit()
        except (IOError, csv.Error, SQLAlchemyError) as e:
            raise FileLoadError(f"Error copying data from {path.name}: {e}") from e
        finally:
            # Manually close the cursor and connection to prevent resource leaks
            if cur is not None:
                cur.close()
            if raw_conn is not None:
                raw_conn.close()

    def _create_copy_generator(
        self, file_handle: TextIO, table_columns: List[str]
    ) -> Generator[str, None, None]:
        """
        A generator that reads a csv file, remaps columns on the fly, and yields
        lines in a format suitable for the COPY command.
        :param file_handle: An open text file handle for the source CSV.
        :param table_columns: The list of columns for the target database table
        Yields:
            Formatted strings for each row to be loaded.
        """
        reader = csv.reader(file_handle)
        try:
            file_header = next(reader)
        except StopIteration:
            logging.warning(f"Skipping empty file: {file_handle.name}")
            return

        # Guard against duplicate headers in the source file
        normalized_headers = [self._normalize_identifier(h) for h in file_header]
        if len(normalized_headers) != len(set(normalized_headers)):
            logging.warning(
                f"Duplicate column headers found in the file {file_handle.name} after normalization. This may lead to incorrect data mapping"
            )

        file_header_map = {norm_h: i for i, norm_h in enumerate(normalized_headers)}

        index_map: List[Optional[int]] = [
            file_header_map.get(col) for col in table_columns
        ]

        yield ",".join(f'"{c}"' for c in table_columns) + "\n"

        for row in reader:
            output_row = []
            for idx in index_map:
                if idx is None or idx >= len(row):
                    val = ""
                else:
                    val = row[idx] if row[idx] is not None else ""
                escaped_val = str(val).replace('"', '""')
                output_row.append(f'"{escaped_val}"')
            yield ",".join(output_row) + "\n"

    @staticmethod
    def _read_and_parse_schema(path: Path) -> Tuple[Path, Dict[str, str]]:
        """
        Reads and validates a schema JSON file.
        :param path: The path to the schema JSON file.
        :return:
            A tuple containing the normalized table name and a dictionary of
            normalized column names to their original SQL types.
        """
        try:
            meta = json.loads(path.read_text())
            source_csv = meta.get("source_csv")
            if not source_csv:
                raise SchemaValidationError(
                    "Schema JSON must contain a 'source_csv' key."
                )

            table_name = PostgresLoader._normalize_identifier(
                meta.get("table_name") or Path(source_csv).stem
            )

            schema_data = meta.get("schema")
            if not isinstance(schema_data, dict):
                raise SchemaValidationError(
                    "'schema' key must contain a dictionary of columns."
                )

            cols = {
                PostgresLoader._normalize_identifier(k): v
                for k, v in schema_data.items()
            }
            return table_name, cols
        except (json.JSONDecodeError, KeyError, TypeError) as e:
            raise SchemaValidationError(f"Invalid schema file {path.name}: {e}") from e

    @staticmethod
    def _find_matching_files(
        table_name: str, sjson_name: str, files: List[Path]
    ) -> List[Path]:
        """
        Finds data files that are likely associated with a given table name.
        :param table_name: The name of the table.
        :param sjson_name: The name of the schema file being processed.
        :param files: A list of all available data files.
        :return:
            A list of candidate file paths.
        """
        candidates: List[Path] = [p for p in files if table_name in p.stem.lower()]
        if candidates:
            return candidates

        key = (
            "accepted"
            if "accepted" in sjson_name.lower()
            else ("rejected" if "rejected" in sjson_name.lower() else None)
        )
        if key:
            return [p for p in files if key in p.name.lower()]

        return []

    @staticmethod
    def _list_tabular_files() -> List[Path]:
        """
        Returns a sorted list of all CSV and gzipped CSV files
        in the raw data directory.
        :return:
        """
        return sorted(
            [
                p
                for p in RAW_DIR.rglob("*")
                if p.is_file()
                and (
                    p.name.lower().endswith(".csv")
                    or p.name.lower().endswith(".csv.gz")
                )
            ]
        )

    @staticmethod
    def _normalize_identifier(name: str) -> str:
        """Converts a string into a valid, lowercase SQL identifier."""
        if not name:
            return "_empty_"
        n = name.strip().lower()
        n = re.sub(r"\s+", "-", n)
        n = re.sub(r"[^a-z0-9_]", "_", n)
        if re.match(r"^\d", n):
            n = f"c_{n}"
        return n

    @staticmethod
    def _quote_ident(name: str) -> str:
        """
        Quotes an SQL identifier to handle special characters and
        reserved words
        :param name:
        :return:
        """
        return '"' + name.replace('"', '""') + '"'


# Main Execution
def get_db_engine() -> Engine:
    """
    Loads database configuration from env variables and creates
    SQLAlchemy engine instance.
    :return:
        An SQLAlchemy engine instance.
    """
    load_dotenv()
    try:
        user = os.environ["POSTGRES_USER"]
        pwd = os.environ["POSTGRES_PASSWORD"]
        host = os.environ["POSTGRES_HOST"]
        port = os.environ["POSTGRES_PORT"]
        db = os.environ["POSTGRES_DB"]
        url = f"postgresql+psycopg2://{user}:{pwd}@{host}:{port}/{db}"
        logging.info(f"Connecting to PostgreSQL at {host}:{port}")
        return create_engine(url)
    except KeyError as e:
        raise EnvironmentError(f"Missing required environment variable: {e}")


def main():
    logging.info("Starting PostgreSQL data loading process.")
    try:
        engine = get_db_engine()
        loader = PostgresLoader(engine)

        loader.ensure_db_schema_exists(["raw", "staging", "features"])
        loader.load_all_tables()

        logging.info("--- Raw data load complete. ---")

    except (PostgresLoaderError, FileNotFoundError, EnvironmentError) as e:
        logging.error(f" A critical error occurred: {e}")
    except Exception as e:
        logging.error(
            f"An unexpected and unhandled error occurred : {e}", exc_info=True
        )


if __name__ == "__main__":
    main()
